\section{Introduction and previous research}

One of the central assumptions in node classification tasks is that neighboring nodes have similar classes. This has been extensively used in node classification tasks, and in machine learning based approaches to predict node classes. Such approaches are now often denoted as graph machine learning or graph neural networks (i.e. machine learning where the input is a graph/network, in contrast with neural network, where the network topology is not directly related to the observations). Three main approaches have been proposed to take advantage of a graph in machine learning:

\begin{itemize}
\item   Regularization of the output requiring that neighboring nodes should have similar classes, and graph partitioning. Such methods often include partitioning the graphs based on the eigenvalues of the Laplacian or weighted Laplacian \cite{dhillon2007weighted,karypis1995analysis}. Other works have used variants of this idea, each using smoothness and graph distance (e.g.,\cite{belkin2004semi,sindhwani2005linear}). An alternative approach is to use quadratic penalty on the difference between neighboring nodes \cite{zhou2004learning,zhu2003semi}.
\item   Graph-based label propagation. Multiple diffusion and information propagation models have been proposed \cite{rosenfeld2017semi}. For example, DeepWalk \cite{perozzi2014deepwalk}, where a truncated random walk is performed on nodes and used to project nodes to a multidimensional real space. Planetoid \cite{yang2016revisiting} also uses random walks combined with negative sampling. Duvenaud et al. used a translation of subgraphs to hash functions for a similar task in the context of molecule classifications \cite{duvenaud2015convolutional}. A very similar approach was presented by Leskovech by projecting nodes minimizing the distance of neighbored nodes in a truncated random walk (Node2vec \cite{grover2016node2vec}). The DNGR model \cite{cao2016deep} uses random walk to compute the mutual information between points A Multi-Dimensional-Scaling (MDS) projection of the points in the graphs was also used for a similar goal \cite{belkin2002laplacian,levy2015improving}. An alternative approach was inspired by word embedding methods \cite{mikolov2013distributed} such as word2vec. These methods use the graph to define a “context” in relation to which the node embedding is constructed. When the data includes only the graph, the embeddings are used as features and fed into existing predictors \cite{perozzi2014deepwalk}. When the data includes node features, these embeddings are used as an additional regularization term to a standard loss over the labeled nodes \cite{yang2016revisiting}. Refex \cite{henderson2011s} defines local features to translate each node to a vector features and use those to predict classes.
\item   Graph Convolution Networks (GCN) to learn the relation between the input of a node and its neighbors to its class. Recently, Kipfs and collaborators, in a seminal work, proposed a simplification of spectral based convolutions \cite{kipf2016semi,schlichtkrull2018modeling}, and instead use a two-layer approach, where the weights of each layer are multiplied by a derivative of the adjacency matrix. They tested their work on multiple graphs with labeled nodes including CiteSeer, Cora Pubmed and Nell. GCN approaches can also be used where the graph is used as a filter on the input. Most such convolutions are spectral (use the Laplacian eigenvectors). However, recent methods are based on random filters \cite{atwood2016diffusion,bruna2013spectral,henaff2015deep}. Similar formalisms were used to study not only single snapshots but also time series of graphs, mainly in image analysis \cite{seo2018structured}.

\end{itemize}

These methods presume a single graph or a single multi-graphs and static node classifications. However, in many realistic scenarios, both the graph and the labels evolve. An interesting example would be companies labeled as successful or not, and graphs representing the relation between such companies. We here analyze a company similarity network over 17 years and show that a combination of GCN with LSTM can improve the accuracy of the prediction of success and bankruptcies.
